{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Modelagem de Classificação / Classification Modeling\n",
    "\n",
    "Este notebook constrói um pipeline supervisionado com validação cruzada, tuning de hiperparâmetros, explicabilidade (SHAP/feature importance), e avaliação avançada (métricas e curvas ROC/PR).\n",
    "\n",
    "This notebook builds a supervised classification pipeline with cross-validation, hyperparameter tuning, explainability (SHAP/feature importance), and advanced evaluation (metrics and ROC/PR curves)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports e configuração / Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install scikit-learn xgboost shap matplotlib seaborn numpy pandas --quiet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, roc_curve, auc,\n",
    "                            precision_recall_curve, average_precision_score)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "sns.set(style='whitegrid', context='talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dados / Data\n",
    "\n",
    "Substitua esta célula para carregar seus dados. Abaixo usamos o dataset Breast Cancer (sklearn) como exemplo.\n",
    "Replace this cell to load your data. Below we use the Breast Cancer dataset (sklearn) as an example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame.copy()\n",
    "target_col = 'target'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Split treino/teste / Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Pré-processamento / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ], remainder='drop'\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Modelo base / Base model\n",
    "\n",
    "Escolha entre RandomForest e XGBoost alterando o pipeline abaixo.\n",
    "Choose between RandomForest and XGBoost by editing the pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "use_xgb = True  # mude para False para usar RandomForest / change to False for RandomForest\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=300, max_depth=None)\n",
    "xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "estimator = xgb if use_xgb else rf\n",
    "pipe = Pipeline(steps=[('pre', preprocessor), ('clf', estimator)])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Validação cruzada e tuning / Cross-validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "if use_xgb:\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [300, 500],\n",
    "        'clf__max_depth': [3, 4, 5],\n",
    "        'clf__learning_rate': [0.03, 0.05, 0.1],\n",
    "        'clf__subsample': [0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "else:\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [200, 400, 800],\n",
    "        'clf__max_depth': [None, 5, 10],\n",
    "        'clf__max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe, param_grid=param_grid, cv=cv, n_jobs=-1, scoring='roc_auc', verbose=1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best AUC-ROC (CV):', grid.best_score_)\n",
    "print('Best params:', grid.best_params_)\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Avaliação no teste / Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_proba))\n",
    "print('Average Precision (PR AUC):', average_precision_score(y_test, y_proba))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de confusão / Confusion matrix')\n",
    "plt.xlabel('Predito / Predicted')\n",
    "plt.ylabel('Verdadeiro / True')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
    "ap = average_precision_score(y_test, y_proba)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f'AP = {ap:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precisão-Recall / Precision-Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Explicabilidade / Explainability (Feature Importance, SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importâncias de features (para modelos tree-based)\n",
    "final_clf = best_model.named_steps['clf']\n",
    "feature_names = numeric_features + categorical_features\n",
    "if hasattr(final_clf, 'feature_importances_'):\n",
    "    importances = final_clf.feature_importances_\n",
    "    imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(20)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=imp.values, y=imp.index)\n",
    "    plt.title('Top feature importances')\n",
    "    plt.show()\n",
    "\n",
    "# SHAP (pode ser custoso)\n",
    "# SHAP (can be expensive)\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(final_clf)\n",
    "    # Transform train to model input space\n",
    "    X_train_trans = best_model.named_steps['pre'].transform(X_train)\n",
    "    shap_values = explainer.shap_values(X_train_trans)\n",
    "    shap.summary_plot(shap_values, X_train_trans, feature_names=feature_names, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('SHAP skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Exportação de resultados / Results export"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Probabilidades e predições\n",
    "preds = pd.DataFrame({\n",
    "    'y_true': y_test.values,\n",
    "    'y_proba': y_proba,\n",
    "    'y_pred': y_pred\n",
    "})\n",
    "preds.to_csv('predicoes_test.csv', index=False)\n",
    "\n",
    "# Melhor conjunto de hiperparâmetros\n",
    "pd.Series(grid.best_params_).to_json('best_params.json')\n",
    "\n",
    "# Importâncias (se disponíveis)\n",
    "if 'importances' in locals():\n",
    "    imp.to_csv('feature_importances_top20.csv')\n",
    "\n",
    "print('Arquivos exportados: predicoes_test.csv, best_params.json, feature_importances_top20.csv (se aplicável)')\n",
    "print('Exported files: predicoes_test.csv, best_params.json, feature_importances_top20.csv (if applicable)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Notas finais / Final notes\n",
    "- Adapte o pré-processamento às suas variáveis.\n",
    "- Para dados desbalanceados, considere class_weight, thresholds ou técnicas de reamostragem.\n",
    "- Troque o scoring conforme objetivo (f1, average_precision, etc.).\n",
    "- Pipeline e GridSearch evitam vazamento de dados.\n",
    "\n",
    "- Adapt preprocessing to your variables.\n",
    "- For imbalanced data, consider class_weight, thresholds, or resampling.\n",
    "- Change scoring as needed (f1, average_precision, etc.).\n",
    "- Pipeline and GridSearch prevent data leakage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
