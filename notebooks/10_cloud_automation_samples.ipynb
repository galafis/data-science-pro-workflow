{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Cloud Automation Samples / Amostras de Automação em Nuvem\n",
    "\n",
    "This bilingual, hands-on notebook shows end-to-end snippets for: Google Cloud, AWS, and Azure. Topics include: model deployment, object storage (buckets), remote execution, CI/CD automation, and security tips.\n",
    "\n",
    "Este notebook bilíngue e prático traz trechos de ponta a ponta para: Google Cloud, AWS e Azure. Tópicos incluem: deploy de modelos, armazenamento em buckets, execução remota, automação CI/CD e dicas de segurança."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites / Pré-requisitos\n",
    "- Python 3.10+\n",
    "- Credentials configured locally (gcloud, aws, az CLIs) or environment variables set.\n",
    "- Appropriate IAM/permissions for the actions.\n",
    "\n",
    "- Credenciais configuradas localmente (gcloud, aws, az) ou variáveis de ambiente.\n",
    "- Permissões/IAM adequadas para as ações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Google Cloud: Storage, Cloud Run deploy, and Batch remote job\n",
    "## 1) Google Cloud: Storage, deploy no Cloud Run e job remoto no Batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {"execution": {}},
   "source": [
    "# Install and imports (uncomment as needed)\n",
    "# !pip install google-cloud-storage google-cloud-aiplatform google-cloud-run google-cloud-batch google-auth\n",
    "from google.cloud import storage\n",
    "import os, json, subprocess\n",
    "\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'your-project-id')\n",
    "BUCKET = f'{PROJECT_ID}-ds-artifacts'\n",
    "REGION = os.getenv('GOOGLE_CLOUD_REGION', 'us-central1')\n",
    "SERVICE_NAME = 'demo-ml-api'\n",
    "IMAGE = f'gcr.io/{PROJECT_ID}/{SERVICE_NAME}:latest'\n",
    "\n",
    "# Create bucket if not exists / Cria bucket se não existir\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "buckets = [b.name for b in client.list_buckets()]\n",
    "if BUCKET not in buckets:\n",
    "    client.create_bucket(BUCKET, location=REGION)\n",
    "    print('Created bucket:', BUCKET)\n",
    "else:\n",
    "    print('Bucket exists:', BUCKET)\n",
    "\n",
    "# Upload a model artifact / Envia um artefato de modelo\n",
    "blob_path = 'models/sklearn_demo/model.pkl'\n",
    "blob = client.bucket(BUCKET).blob(blob_path)\n",
    "blob.upload_from_filename('model.pkl')\n",
    "print('Uploaded to gs://%s/%s' % (BUCKET, blob_path))\n",
    "\n",
    "# Build & push image (requires gcloud auth) / Build e push de imagem\n",
    "subprocess.run(['gcloud','builds','submit','--tag', IMAGE], check=True)\n",
    "\n",
    "# Deploy to Cloud Run / Deploy no Cloud Run\n",
    "subprocess.run(['gcloud','run','deploy', SERVICE_NAME, '--image', IMAGE, '--region', REGION, '--platform','managed','--allow-unauthenticated'], check=True)\n",
    "\n",
    "# Submit remote job to Batch (example) / Submeter job remoto ao Batch (exemplo)\n",
    "# Note: requires a job spec JSON file / requer um spec JSON\n",
    "# subprocess.run(['gcloud','batch','jobs','submit','demo-batch','--location', REGION, '--config','batch_job.json'], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) AWS: S3, SageMaker model deploy, and Batch job\n",
    "## 2) AWS: S3, deploy no SageMaker e job no Batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !pip install boto3 sagemaker\n",
    "import boto3, os\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name or os.getenv('AWS_REGION','us-east-1')\n",
    "s3 = boto3.client('s3', region_name=region)\n",
    "bucket = os.getenv('AWS_S3_BUCKET','your-ds-artifacts')\n",
    "key = 'models/sklearn_demo/model.pkl'\n",
    "s3.create_bucket(Bucket=bucket) if bucket not in [b['Name'] for b in s3.list_buckets()['Buckets']] else None\n",
    "s3.upload_file('model.pkl', bucket, key)\n",
    "print(f'Uploaded to s3://{bucket}/{key}')\n",
    "\n",
    "# SageMaker deploy (high-level example)\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "role = os.getenv('AWS_SAGEMAKER_ROLE','arn:aws:iam::123456789012:role/SageMakerExecutionRole')\n",
    "model = SKLearnModel(model_data=f's3://{bucket}/{key}', role=role, entry_point='inference.py', framework_version='1.2-1')\n",
    "predictor = model.deploy(instance_type='ml.m5.large', initial_instance_count=1)\n",
    "print('Endpoint name:', predictor.endpoint_name)\n",
    "\n",
    "# AWS Batch job submit (requires a Job Queue/Definition)\n",
    "batch = boto3.client('batch', region_name=region)\n",
    "# batch.submit_job(jobName='demo-batch', jobQueue='your-queue', jobDefinition='your-def', containerOverrides={'command':['python','task.py']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Azure: Blob Storage, Azure ML Online Endpoint, and Batch\n",
    "## 3) Azure: Blob Storage, Azure ML Endpoint Online e Batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !pip install azure-storage-blob azure-ai-ml azure-identity\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.ml import MLClient\n",
    "import os\n",
    "cred = DefaultAzureCredential()\n",
    "subscription_id = os.getenv('AZURE_SUBSCRIPTION_ID')\n",
    "resource_group = os.getenv('AZURE_RESOURCE_GROUP')\n",
    "workspace = os.getenv('AZURE_ML_WORKSPACE')\n",
    "ml = MLClient(cred, subscription_id, resource_group, workspace)\n",
    "\n",
    "# Blob Storage upload\n",
    "conn_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "container = os.getenv('AZURE_BLOB_CONTAINER','ds-artifacts')\n",
    "bsc = BlobServiceClient.from_connection_string(conn_str)\n",
    "try:\n",
    "    bsc.create_container(container)\n",
    "except Exception:\n",
    "    pass\n",
    "bsc.get_container_client(container).upload_blob('models/sklearn_demo/model.pkl', data=open('model.pkl','rb'), overwrite=True)\n",
    "print('Uploaded to Azure Blob')\n",
    "\n",
    "# Azure ML: create online endpoint (YAML/specs required)\n",
    "# ml.online_endpoints.begin_create_or_update(Endpoint(...))\n",
    "# ml.online_deployments.begin_create_or_update(Deployment(...))\n",
    "# ml.online_endpoints.invoke(endpoint_name, request_file='payload.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) CI/CD automation examples (GitHub Actions)\n",
    "## 4) Exemplos de automação CI/CD (GitHub Actions)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# .github/workflows/deploy.yml (snippet)\n",
    "deploy_yaml = '''\n",
    "name: CI-CD-Cloud\n",
    "on:\n",
    "  push:\n",
    "    branches: [ \"main\" ]\n",
    "jobs:\n",
    "  build-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      - name: Install deps\n",
    "        run: pip install -r requirements.txt\n",
    "      - name: Build image\n",
    "        run: docker build -t ${{ secrets.GCP_IMAGE }} .\n",
    "      - name: Auth GCP\n",
    "        uses: google-github-actions/auth@v2\n",
    "        with:\n",
    "          credentials_json: ${{ secrets.GCP_SA_KEY }}\n",
    "      - name: Push & Deploy Cloud Run\n",
    "        run: |\n",
    "          gcloud auth configure-docker gcr.io -q\n",
    "          docker push ${{ secrets.GCP_IMAGE }}\n",
    "          gcloud run deploy demo-ml-api --image ${{ secrets.GCP_IMAGE }} --region us-central1 --platform managed --allow-unauthenticated\n",
    "'''\n",
    "print(deploy_yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Security tips / Dicas de segurança\n",
    "- Never hardcode secrets. Use Secret Managers (GCP Secret Manager, AWS Secrets Manager, Azure Key Vault).\n",
    "- Principle of least privilege for service accounts and roles.\n",
    "- Enable artifact scanning and image vulnerability checks.\n",
    "- Network policies and private endpoints where possible.\n",
    "- Rotate credentials regularly and enforce MFA.\n",
    "\n",
    "- Nunca codifique segredos no código. Use gerenciadores de segredos.\n",
    "- Princípio do menor privilégio para contas/roles.\n",
    "- Habilite varredura de artefatos e imagens.\n",
    "- Políticas de rede e endpoints privados quando possível.\n",
    "- Rode rotação de credenciais e MFA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) References / Referências\n",
    "- Google Cloud: Cloud Run, Storage, Batch, Vertex AI: https://cloud.google.com/\n",
    "- AWS: S3, SageMaker, Batch: https://aws.amazon.com/\n",
    "- Azure: Blob Storage, Azure ML, Batch: https://azure.microsoft.com/\n",
    "- Terraform for IaC: https://www.terraform.io/\n",
    "- GitHub Actions: https://docs.github.com/actions\n",
    "- Twelve-Factor App: https://12factor.net/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
